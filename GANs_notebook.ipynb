{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train.astype(np.float32) -127.5)/127.5\n",
    "X_train = np.expand_dims(X_train, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building generator\n",
    "def build_generator():\n",
    "  model = tf.keras.Sequential([\n",
    "    Dense(256, input_shape=(100,)),\n",
    "    LeakyReLU(0.2),\n",
    "    Dense(512),\n",
    "    LeakyReLU(0.2),\n",
    "    Dense(1024),\n",
    "    LeakyReLU(0.2),\n",
    "    Dense(784, activation='tanh'),\n",
    "    Reshape((28, 28, 1))\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "#building discriminator\n",
    "def build_discriminator():\n",
    "  model = tf.keras.Sequential([\n",
    "    Dense(512, input_shape=(28, 28, 1)),\n",
    "    LeakyReLU(0.2),\n",
    "    Dense(256),\n",
    "    LeakyReLU(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the loss function\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "#optimization\n",
    "gen_optimization = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimization = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "  noise = tf.random.normal([BATCH_SIZE, 100])\n",
    "\n",
    "  #---Train Discriminator---\n",
    "  with tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, training = True)\n",
    "    \n",
    "    real_output = discriminator(images, training = True)\n",
    "    fake_output = discriminator(generated_images, training = True)\n",
    "\n",
    "    real_loss = cross_entropy_loss(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy_loss(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "    disc_loss = real_loss + fake_loss\n",
    "\n",
    "  disc_grad = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  disc_optimization.apply_gradients(zip(disc_grad, discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "  #---Train Generator---\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "    generated_images = generator(noise, training = True)\n",
    "    fake_output = discriminator(generated_images, training = True)\n",
    "    gen_loss = cross_entropy_loss(tf.ones_like(fake_output), fake_output)\n",
    "  \n",
    "  gen_grad = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "  gen_optimization.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
    "\n",
    "  return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track losses\n",
    "gen_losses = []\n",
    "disc_losses = []\n",
    "\n",
    "#make a dir to save generated images\n",
    "os.makedirs(\"Generated_Images\", exist_ok = True)\n",
    "\n",
    "#Generate images and save them\n",
    "def gen_save_images(model, epoch, test_input):\n",
    "  predictions = model(test_input, training = False)\n",
    "  fig = plt.figure(figsize=(5, 5))\n",
    "  for i in range(predictions.shape[0]):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "  plt.suptitle(f\"Epoch {epoch}\")\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f\"Generated_Images/image_epoch_{epoch:03d}.png\")\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset, epochs=50):\n",
    "  seed = tf.random.normal([25, 100])\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for image_batch in dataset:\n",
    "      gen_loss, disc_loss = train_step(image_batch)\n",
    "\n",
    "    #Track loss\n",
    "    gen_losses.append(gen_loss.numpy())\n",
    "    disc_losses.append(disc_loss.numpy())\n",
    "\n",
    "    #Print epochs\n",
    "    print(f'Epoch: {epoch+1}, Generator Loss: {gen_loss:.4f}, Discriminator Loss: {disc_loss:.4f}')\n",
    "    gen_save_images(generator, epoch+1, seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(dataset, epochs = 100)5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.plot(gen_losses, label='Generator Loss')\n",
    "  plt.plot(disc_losses, label='Discriminator Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Trainin Losses')\n",
    "  plt.legend\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_generated_images():\n",
    "  noise = tf.random.normal([25, 100])\n",
    "  generated_images = generator(noise, training = False)\n",
    "\n",
    "  fig = plt.figure(figsize=(5, 5))\n",
    "  for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(generated_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "  plt.suptitle(\"Final Generated Images\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_generated_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_disc():\n",
    "  noise = tf.random.normal([BATCH_SIZE, 100])\n",
    "  generated_image = generator(noise, training = False)\n",
    "  real_image = next(iter(dataset))\n",
    "\n",
    "  real_output = discriminator(real_image, training = False)\n",
    "  fake_output = discriminator(generated_image, training = False)\n",
    "\n",
    "  real_acc = tf.reduce_mean(tf.cast(real_output > 0.5, tf.float32))\n",
    "  fake_acc = tf.reduce_mean(tf.cast(fake_output < 0.5, tf.float32))\n",
    "\n",
    "  print(f'Real Accuracy: {real_acc:.4f}, Fake Accuracy: {fake_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_disc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
